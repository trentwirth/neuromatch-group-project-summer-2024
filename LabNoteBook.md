# 2024-07-09 

Goal for day 1:
- get to know our groups
- choose a dataset
- brainstorm and search the literature for interesting papers

For the rest of week 1, we will conduct preliminary analyses of the datasets with the goal of refining and informing our research qeustion

by week 2 day 1, we will learn a general strategy for approaching a research project

> Note: I struggled a bit today remembering linear algebra; I'll need to go back and do that refresher!

Day 3/4:
Regularization:
- Use L1 regularization to select features, by setting some coefficients to zero
- Use L2 regularization to prevent overfitting, by penalizing large coefficients (reducing the weight of the coefficients across all features)


Week 2 Day 3:

Bayesian Rings Notes
> “However, real memories often include a sense of uncertainty, (e.g., refs. 5–7), and uncertainty has clear behavioral effects (8–10). This motivates us to ask how an attractor network might conjunctively encode a memory and its associated uncertainty.” 

A scientific problem I (Trent) have with this is that I don’t like motivating models on pure self-report, this idea that we must encode uncertainty into memory representation because people verbally claim that their memory is “uncertain” is iffy to me. This really doesn’t matter for our project, I’m just “thinking out loud” 😀 … They could have just written about the clear degradation of memories over time, and that would have been justification enough. By using the uncertain language, they’re placing themselves in the “Bayesian box.” Perhaps intentionally.

- Is it possible to have a spherical attractor? Like, instead of a ring encoding a 2D vector, it’s a sphere encoding a 3D one? It seems like it would have to be possible lol. I’m just very excited by this prospect for my own personal research. 
    - Is there any advantage to representing 3D space in terms of a 3D spherical attractor? Or is it more efficient to do it in other ways.
